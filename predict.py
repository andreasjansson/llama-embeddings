from typing import List
import os
import time
import subprocess
import json
import numpy as np
from cog import BasePredictor, Input, Path, BaseModel, ConcatenateIterator
import pprint as pp
from llama_cpp import Llama
from tqdm import tqdm


class Predictor(BasePredictor):
    def setup(self):
        # model.txt is generated by the Makefile
        with open("model.txt") as f:
            model = f.read().strip()
        model_path = f"/models/{model}"
        model_url = f"https://storage.googleapis.com/replicate-weights/llamacpp/{model}"
        start = time.time()
        if not os.path.exists(model_path):
            print("Downloading model weights....")
            subprocess.check_call(["pget", model_url, model_path])
            print("Downloading weights took: ", time.time() - start)
        self.llm = Llama(
            model_path,
            n_ctx=2048,
            n_gpu_layers=-1,
            main_gpu=0,
            n_threads=1,
            embedding=True,
            verbose=False,
        )

    def predict(
        self,
        prompts: str = Input(
            description="List of prompts, separated by prompt_separator. Maximum 100 prompts per prediction."
        ),
        prompt_separator: str = Input(
            description="Separator between prompts", default="\n\n"
        ),
    ) -> List[List[float]]:
        prompts = prompts.split(prompt_separator)
        if len(prompts) > 100:
            raise ValueError("The maximum number of prompts per prediction is 100")
        print(f"Generating embeddings for {len(prompts)} prompts")

        outputs = []
        for prompt in tqdm(prompts):
            outputs.append(self.llm.embed(prompt))
        return outputs
